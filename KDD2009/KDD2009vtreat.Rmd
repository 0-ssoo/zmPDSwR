

Practical data science with R built chapter 6 built a number of single variable models.
In Listing 6.11 it used an ad-hoc entropy based out of sample effect size estimate
for variable selection.  This likely (though it isn't completely rigorous) picked 
variables conservatively.

We show here how to repeat this work on the KDD2009 dataset using more standard
techniques more quickly.
For vtreat details see: 
   http://www.win-vector.com/blog/2014/08/vtreat-designing-a-package-for-variable-treatment/
and Chapter 6 of Practical Data Science with R: 
    http://www.amazon.com/Practical-Data-Science/dp/1617291560
For details on data see: 
    https://github.com/WinVector/zmPDSwR/tree/master/KDD2009
There is an issue that any data row used to build the single variable models isn't
exchangable with future unseen rows for the purposes of scoring and training.  So
the most hygienic way to work is to use one subset of data to build the single variable models,
and then another to built the composite model, and a third for scoring.  In particular
models trained using rows used to build sub-models think the sub-models have large effects
that the sub-models will in the future, and under-estimate degrees of freedom of complicated
sub-models.

```{r kddexlibs, tidy=FALSE}
#load some libraries
# http://www.win-vector.com/blog/2014/08/vtreat-designing-a-package-for-variable-treatment/
# devtools::install_github("WinVector/vtreat")
library('vtreat')
# devtools::install_github("WinVector/vtreat")
library('WVPlots')

library('parallel')
library('gbm')
library('class')


# load the data as in the book
# change this path to match your directory structure
dir = '~/Documents/work/PracticalDataScienceWithR/zmPDSwR/KDD2009/' 

d = read.table(paste(dir,'orange_small_train.data.gz',sep=''),
                header=T,sep='\t',na.strings=c('NA',''), 
               stringsAsFactors=FALSE)
churn = read.table(paste(dir,'orange_small_train_churn.labels.txt',sep=''),
                    header=F,sep='\t')
d$churn = churn$V1
appetency = read.table(paste(dir,'orange_small_train_appetency.labels.txt',sep=''),
                        header=F,sep='\t')
d$appetency = appetency$V1
upselling = read.table(paste(dir,'orange_small_train_upselling.labels.txt',sep=''),
                        header=F,sep='\t')
d$upselling = upselling$V1
set.seed(729375)
d$rgroup = runif(dim(d)[[1]])
dTrainM = subset(d,rgroup<=0.5)  # set for building models
dTrainC = subset(d,(rgroup>0.5) & (rgroup<=0.9)) # set for impact coding
dTest = subset(d,rgroup>0.9) # set for evaluation
rm(list=c('d','churn','appetency','upselling','dir'))
outcomes = c('churn','appetency','upselling')
vars = setdiff(colnames(dTrainM),
                c(outcomes,'rgroup'))
yName = 'churn'
yTarget = 1

set.seed(239525)

cl = parallel::makeCluster(4)
```




```{r kddmodels, tidy=FALSE}
# Run other models (with proper coding/training separation).
#
# This gets us back to AUC 0.72 range

kddPrune = 0.05

treatmentsC = designTreatmentsC(dTrainC,
    vars,yName,yTarget,
    smFactor=2.0, 
    parallelCluster=cl)

treatedTrainM = prepare(treatmentsC,
                        dTrainM,
                        pruneSig=kddPrune, 
                        parallelCluster=cl)
selvars = setdiff(colnames(treatedTrainM),yName)
treatedTrainM[[yName]] = treatedTrainM[[yName]]==yTarget

treatedTest = prepare(treatmentsC,
                      dTest,
                      pruneSig=kddPrune, 
                      parallelCluster=cl)
treatedTest[[yName]] = treatedTest[[yName]]==yTarget

# prepare plotting frames
treatedTrainP = treatedTrainM[, yName, drop=FALSE]
treatedTestP = treatedTest[, yName, drop=FALSE]


formulaS = paste(yName,paste(selvars,collapse=' + '),sep=' ~ ')
for(mname in c('gbmPred','glmPred')) {
  print("*****************************")
  print(date())
  print(paste(mname,length(selvars)))
  if(mname=='gbmPred') {
    modelGBMs = gbm(as.formula(formulaS),
                    data=treatedTrainM,
                    distribution='bernoulli',
                    n.trees=500,
                    interaction.depth=3,
                    keep.data=FALSE,
                    cv.folds=5)
    nTrees = gbm.perf(modelGBMs)
    treatedTrainP[[mname]] = predict(modelGBMs,newdata=treatedTrainM,type='response',
                                     n.trees=nTrees) 
    treatedTestP[[mname]] = predict(modelGBMs,newdata=treatedTest,type='response',
                                    n.trees=nTrees)
  } else {
    modelglms = glm(as.formula(formulaS),
                    data=treatedTrainM,
                    family=binomial(link='logit')
    )
    treatedTrainP[[mname]] = predict(modelglms,newdata=treatedTrainM,type='response')
    treatedTestP[[mname]] = predict(modelglms,newdata=treatedTest,type='response')
  }
  
  t1 = paste(mname,'trainingM data')
  print(DoubleDensityPlot(treatedTrainP, mname, yName, 
                          title=t1))
  print(ROCPlot(treatedTrainP, mname, yName, 
                title=t1))
  
  t2 = paste(mname,'test data')
  print(DoubleDensityPlot(treatedTestP, mname, yName, 
                          title=t2))
  print(ROCPlot(treatedTestP, mname, yName, 
                title=t2))
  print(date())
  print("*****************************")
}
saveRDS(treatedTestP,'treatedTestP.rds')
write.table(treatedTestP,row.names = FALSE,col.names = TRUE,sep='\t',file='treatedTestP.tsv')
```

```{r shutdown, tidy=FALSE}
if(!is.null(cl)) {
    parallel::stopCluster(cl)
    cl = NULL
}
```

